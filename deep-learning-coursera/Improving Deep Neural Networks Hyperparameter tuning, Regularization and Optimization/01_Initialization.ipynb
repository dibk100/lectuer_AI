{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "# from init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\n",
    "# from init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    np.random.seed(1)\n",
    "    train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)\n",
    "    np.random.seed(2)\n",
    "    test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)\n",
    "    # Visualize the data\n",
    "    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral);\n",
    "    train_X = train_X.T\n",
    "    train_Y = train_Y.reshape((1, train_Y.shape[0]))\n",
    "    test_X = test_X.T\n",
    "    test_Y = test_Y.reshape((1, test_Y.shape[0]))\n",
    "    return train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image dataset: blue/red dots in circles\n",
    "train_X, train_Y, test_X, test_Y = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - 신경망 모델\n",
    "\n",
    "당신은 3-레이어 신경망을 사용할 것입니다 (이미 구현되어 있음). 다음은 실험하게 될 초기화 방법들입니다:\n",
    "\n",
    "- **Zeros 초기화** — 입력 인자에서 `initialization = \"zeros\"`로 설정합니다.\n",
    "- **Random 초기화** — 입력 인자에서 `initialization = \"random\"`로 설정합니다. 이 방법은 가중치를 큰 랜덤 값으로 초기화합니다.\n",
    "- **He 초기화** — 입력 인자에서 `initialization = \"he\"`로 설정합니다. 이 방법은 2015년 He 외 논문에 따라 가중치를 스케일된 랜덤 값으로 초기화합니다.\n",
    "\n",
    "**지침:** 아래 코드를 빠르게 읽고 실행해보세요. 다음 파트에서는 이 `model()` 함수에서 호출하는 세 가지 초기화 방법을 구현하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# def model(X, Y, learning_rate=0.01, num_iterations=15000, print_cost=True, initialization=\"he\"):\n",
    "#     \"\"\"\n",
    "#     Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "#     Arguments:\n",
    "#     X -- input data, of shape (2, number of examples)\n",
    "#     Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n",
    "#     learning_rate -- learning rate for gradient descent \n",
    "#     num_iterations -- number of iterations to run gradient descent\n",
    "#     print_cost -- if True, print the cost every 1000 iterations\n",
    "#     initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n",
    "    \n",
    "#     Returns:\n",
    "#     parameters -- parameters learnt by the model\n",
    "#     \"\"\"\n",
    "        \n",
    "#     costs = [] # to keep track of the loss\n",
    "#     layers_dims = [X.shape[0], 10, 5, 1]\n",
    "#     last_L = len(layers_dims)-1\n",
    "    \n",
    "#     parameters = initialize_parameters(layers_dims,initialization)\n",
    "    \n",
    "#     for i in range(0, num_iterations):\n",
    "\n",
    "#         # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "#         cache = forward_propagation(X, parameters)\n",
    "#         a = cache['A'+str(last_L)]\n",
    "        \n",
    "#         # Loss\n",
    "#         cost = compute_loss(a, Y)\n",
    "\n",
    "#         # Backward propagation and update parameters.\n",
    "#         parameters = backward_propagation_update(a, Y, cache,learning_rate)\n",
    "        \n",
    "#         # Print the loss every 1000 iterations\n",
    "#         if print_cost and i % 1000 == 0:\n",
    "#             print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "#             costs.append(cost)\n",
    "            \n",
    "#     # plot the loss\n",
    "#     plt.plot(costs)\n",
    "#     plt.ylabel('cost')\n",
    "#     plt.xlabel('iterations (per hundreds)')\n",
    "#     plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "#     plt.show()\n",
    "    \n",
    "#     return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from re_utils import forward_propagation_for_predict\n",
    "# def predict(X, y, parameters):\n",
    "#     \"\"\"\n",
    "#     This function is used to predict the results of a  n-layer neural network.\n",
    "    \n",
    "#     Arguments:\n",
    "#     X -- data set of examples you would like to label\n",
    "#     parameters -- parameters of the trained model\n",
    "    \n",
    "#     Returns:\n",
    "#     p -- predictions for the given dataset X\n",
    "#     \"\"\"\n",
    "    \n",
    "#     m = X.shape[1]\n",
    "#     p = np.zeros((1,m), dtype = np.int)     # 저장할 파라미터\n",
    "    \n",
    "#     # Forward propagation\n",
    "#     aL = forward_propagation_for_predict(X, parameters)\n",
    "    \n",
    "#     # convert probas to 0/1 predictions\n",
    "#     for i in range(0, aL.shape[1]):\n",
    "#         if aL[0,i] > 0.5:\n",
    "#             p[0,i] = 1\n",
    "#         else:\n",
    "#             p[0,i] = 0\n",
    "\n",
    "#     # print results\n",
    "#     print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n",
    "    \n",
    "#     return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re_utils import compute_loss, forward_propagation, backward_propagation_update,initialize_parameters,forward_propagation_for_predict, predict_re, model_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Zero 초기화\n",
    "\n",
    "신경망에서 초기화해야 할 파라미터는 두 가지 종류가 있습니다:\n",
    "- 가중치 행렬 $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n",
    "- 편향 벡터 $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$\n",
    "\n",
    "**연습 문제**: 모든 파라미터를 0으로 초기화하는 다음 함수를 구현해보세요.  \n",
    "나중에 보게 되겠지만, 이렇게 하면 \"대칭성을 깨지 못하기 때문에\" 잘 작동하지 않습니다.  \n",
    "하지만 일단 시도해보고 어떤 일이 일어나는지 확인해봅시다.  \n",
    "`np.zeros((..,..))`를 사용하되, 올바른 형태(shape)를 사용하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters([3,2,1],'zeros')\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to train your model on 15,000 iterations using zeros initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model_re(train_X, train_Y, initialization = \"zeros\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict_re(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict_re(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기까지\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"zeros\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is really bad, and the cost does not really decrease, and the algorithm performs no better than random guessing. Why? Lets look at the details of the predictions and the decision boundary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"predictions_train = \" + str(predictions_train))\n",
    "print(\"predictions_test = \" + str(predictions_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with Zeros initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 1.5])\n",
    "axes.set_ylim([-1.5, 1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 모든 예제에 대해 0만 예측하고 있습니다.\n",
    "\n",
    "일반적으로, 모든 가중치를 0으로 초기화하면 네트워크가 대칭성을 깨는 데 실패하게 됩니다.  \n",
    "이는 각 층의 모든 뉴런이 동일한 것을 학습하게 된다는 것을 의미하며,  \n",
    "결국 각 층에서 $n^{[l]}=1$인 신경망을 학습하는 것과 다를 바 없게 됩니다.  \n",
    "이러한 경우, 신경망은 로지스틱 회귀와 같은 선형 분류기보다 더 강력하지 않게 됩니다.\n",
    "\n",
    "<font color='blue'>\n",
    "\n",
    "**기억해야 할 것**:\n",
    "- 가중치 $W^{[l]}$는 대칭성을 깨기 위해 랜덤하게 초기화되어야 합니다.  \n",
    "- 하지만 편향 $b^{[l]}$는 0으로 초기화해도 괜찮습니다.  \n",
    "  $W^{[l]}$만 랜덤하게 초기화된다면, 대칭성은 여전히 깨지기 때문입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Random 초기화\n",
    "\n",
    "대칭성을 깨기 위해, 가중치를 랜덤하게 초기화해봅시다.  \n",
    "랜덤 초기화를 통해 각 뉴런은 입력에 대해 서로 다른 함수를 학습할 수 있게 됩니다.  \n",
    "이번 연습에서는, 가중치를 매우 큰 값으로 랜덤하게 초기화했을 때 어떤 일이 일어나는지 확인하게 됩니다.\n",
    "\n",
    "**연습 문제**: 다음 함수를 구현하여 가중치를 큰 랜덤 값(10배 스케일링)으로 초기화하고, 편향은 0으로 초기화하세요.  \n",
    "가중치에는 `np.random.randn(.., ..) * 10`을, 편향에는 `np.zeros((.., ..))`을 사용하세요.  \n",
    "우리는 `np.random.seed(..)`를 고정하여 \"랜덤한\" 가중치가 항상 동일하게 생성되도록 했습니다.  \n",
    "따라서 여러 번 실행해도 초기 파라미터 값이 항상 같더라도 걱정하지 마세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_random\n",
    "\n",
    "def initialize_parameters_random(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)               # This seed makes sure your \"random\" numbers will be the as ours\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # integer representing the number of layers\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 10\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_random([3, 2, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 17.88628473   4.36509851   0.96497468]\n",
    " [-18.63492703  -2.77388203  -3.54758979]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-0.82741481 -6.27000677]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to train your model on 15,000 iterations using random initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"random\")\n",
    "print(\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print(\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iteration 0 이후에 비용(cost)이 \"inf\"로 표시된다면,  \n",
    "이는 수치적인 반올림 오차(numerical roundoff) 때문입니다.  \n",
    "좀 더 정교한 수치 해석 구현이라면 이를 해결할 수 있겠지만,  \n",
    "우리의 목적에서는 굳이 걱정할 필요는 없습니다.\n",
    "\n",
    "어쨌든, 대칭성은 깨졌고, 이전보다 더 나은 결과를 보여주고 있습니다.  \n",
    "모델이 더 이상 모든 예제에 대해 0만 출력하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_train)\n",
    "print(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model with large random initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 1.5])\n",
    "axes.set_ylim([-1.5, 1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**관찰 결과**:\n",
    "- 비용(cost)이 매우 높은 값에서 시작합니다.  \n",
    "  이는 가중치가 큰 랜덤 값으로 초기화되었기 때문인데,  \n",
    "  이 경우 마지막 활성화 함수(sigmoid)가 일부 예제에 대해 0 또는 1에 매우 가까운 값을 출력하게 됩니다.  \n",
    "  이때 예측이 틀리면 해당 예제에 대해 매우 큰 손실(loss)이 발생하게 됩니다.  \n",
    "  실제로 $\\log(a^{[3]}) = \\log(0)$일 경우, 손실은 무한대로 발산합니다.\n",
    "  \n",
    "- 잘못된 초기화는 **그래디언트 소실/폭발(vanishing/exploding gradients)**을 일으킬 수 있으며,  \n",
    "  이는 최적화 알고리즘을 느리게 만듭니다.\n",
    "\n",
    "- 이 네트워크를 더 오래 학습시키면 더 나은 결과를 얻을 수는 있지만,  \n",
    "  지나치게 큰 랜덤 수로 초기화하는 것은 최적화를 느리게 만듭니다.\n",
    "\n",
    "<font color='blue'>\n",
    "\n",
    "**요약**:\n",
    "- 가중치를 매우 큰 랜덤 값으로 초기화하는 것은 좋지 않습니다.\n",
    "- 작은 랜덤 값으로 초기화하면 더 나을 수 있습니다.  \n",
    "  여기서 중요한 질문은: *이 랜덤 값들은 얼마나 작아야 할까?*  \n",
    "  다음 파트에서 알아봅시다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - He 초기화\n",
    "\n",
    "마지막으로, **He 초기화**를 시도해봅시다.  \n",
    "이 방법은 2015년 He 외 연구의 제1저자 이름을 따서 명명되었습니다.  \n",
    "(\"Xavier 초기화\"를 들어본 적이 있다면, 이와 비슷한 방식입니다.  \n",
    "단, Xavier 초기화는 가중치 $W^{[l]}$에 `sqrt(1./layers_dims[l-1])` 스케일링을 사용하는 반면,  \n",
    "He 초기화는 `sqrt(2./layers_dims[l-1])`을 사용합니다.)\n",
    "\n",
    "**연습 문제**: 다음 함수를 구현하여 He 초기화를 이용해 파라미터를 초기화하세요.\n",
    "\n",
    "**힌트**: 이 함수는 이전에 만들었던 `initialize_parameters_random(...)` 함수와 거의 유사합니다.  \n",
    "차이점은 `np.random.randn(.., ..)`에 10을 곱하는 대신,  \n",
    "이전 층의 크기를 기반으로 한 $\\sqrt{\\frac{2}{\\text{이전 층의 차원}}}$을 곱한다는 점입니다.  \n",
    "이는 ReLU 활성화 함수를 사용하는 층에 대해 He 초기화에서 권장하는 방식입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_he\n",
    "\n",
    "def initialize_parameters_he(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * np.sqrt(2 / layers_dims[l - 1])\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_he([2, 4, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **W1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 1.78862847  0.43650985]\n",
    " [ 0.09649747 -1.8634927 ]\n",
    " [-0.2773882  -0.35475898]\n",
    " [-0.08274148 -0.62700068]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b1**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **W2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]\n",
    "    </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td>\n",
    "    **b2**\n",
    "    </td>\n",
    "        <td>\n",
    "    [[ 0.]]\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to train your model on 15,000 iterations using He initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"he\")\n",
    "print(\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print(\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Model with He initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5, 1.5])\n",
    "axes.set_ylim([-1.5, 1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The model with He initialization separates the blue and the red dots very well in a small number of iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5 - Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You have seen three different types of initializations. For the same number of iterations and same hyperparameters the comparison is:\n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "        <td>\n",
    "        **Model**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Train accuracy**\n",
    "        </td>\n",
    "        <td>\n",
    "        **Problem/Comment**\n",
    "        </td>\n",
    "\n",
    "    </tr>\n",
    "        <td>\n",
    "        3-layer NN with zeros initialization\n",
    "        </td>\n",
    "        <td>\n",
    "        50%\n",
    "        </td>\n",
    "        <td>\n",
    "        fails to break symmetry\n",
    "        </td>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with large random initialization\n",
    "        </td>\n",
    "        <td>\n",
    "        83%\n",
    "        </td>\n",
    "        <td>\n",
    "        too large weights \n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "        3-layer NN with He initialization\n",
    "        </td>\n",
    "        <td>\n",
    "        99%\n",
    "        </td>\n",
    "        <td>\n",
    "        recommended method\n",
    "        </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**이 노트북에서 기억해야 할 것**:\n",
    "- 초기화 방식에 따라 결과가 달라질 수 있습니다.\n",
    "- 랜덤 초기화는 대칭성을 깨고, 서로 다른 은닉 유닛들이 서로 다른 것을 학습할 수 있도록 도와줍니다.\n",
    "- 너무 큰 값으로 초기화하지 마세요.\n",
    "- He 초기화는 ReLU 활성화 함수를 사용하는 네트워크에서 좋은 성능을 보입니다."
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "XOESP",
   "launcher_item_id": "8IhFN"
  },
  "kernelspec": {
   "display_name": "diwork_latest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
